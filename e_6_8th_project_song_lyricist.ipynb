{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "revised-acrylic",
   "metadata": {},
   "source": [
    "# 6-7. 프로젝트: 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-provincial",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-transfer",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "copyrighted-perception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-26 11:05:30--  https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
      "Resolving aiffelstaticprd.blob.core.windows.net (aiffelstaticprd.blob.core.windows.net)... 52.239.148.4\n",
      "Connecting to aiffelstaticprd.blob.core.windows.net (aiffelstaticprd.blob.core.windows.net)|52.239.148.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2101791 (2.0M) [application/zip]\n",
      "Saving to: ‘song_lyrics.zip’\n",
      "\n",
      "song_lyrics.zip     100%[===================>]   2.00M  1.54MB/s    in 1.3s    \n",
      "\n",
      "2021-01-26 11:05:31 (1.54 MB/s) - ‘song_lyrics.zip’ saved [2101791/2101791]\n",
      "\n",
      "Archive:  song_lyrics.zip\n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/Kanye_West.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/Lil_Wayne.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/adele.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/al-green.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/alicia-keys.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/amy-winehouse.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/beatles.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/bieber.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/bjork.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/blink-182.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/bob-dylan.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/bob-marley.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/britney-spears.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/bruce-springsteen.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/bruno-mars.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/cake.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/dickinson.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/disney.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/dj-khaled.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/dolly-parton.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/dr-seuss.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/drake.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/eminem.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/janisjoplin.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/jimi-hendrix.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/johnny-cash.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/joni-mitchell.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/kanye-west.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/kanye.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/lady-gaga.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/leonard-cohen.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/lil-wayne.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/lin-manuel-miranda.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/lorde.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/ludacris.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/michael-jackson.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/missy-elliott.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/nickelback.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/nicki-minaj.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/nirvana.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/notorious-big.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/notorious_big.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/nursery_rhymes.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/patti-smith.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/paul-simon.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/prince.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/r-kelly.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/radiohead.txt  \n",
      "  inflating: /home/aiffel-dj24/aiffel/lyricist/data/lyrics/rihanna.txt  \n"
     ]
    }
   ],
   "source": [
    "!wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "!unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics  # lyrics 폴더에 압축풀기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-portfolio",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-capacity",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unauthorized-chicken",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['Load up on guns, bring your friends', \"It's fun to lose and to pretend\", \"She's over-bored and self-assured\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-stick",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-astrology",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-martial",
   "metadata": {},
   "source": [
    "### 공백 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "convinced-shuttle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load up on guns, bring your friends\n",
      "It's fun to lose and to pretend\n",
      "She's over-bored and self-assured\n",
      "Oh no, I know a dirty word Hello, hello, hello, how low\n",
      "Hello, hello, hello, how low\n",
      "Hello, hello, hello, how low\n",
      "Hello, hello, hello With the lights out, it's less dangerous\n",
      "Here we are now, entertain us\n",
      "I feel stupid and contagious\n",
      "Here we are now, entertain us\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    #if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-battle",
   "metadata": {},
   "source": [
    "### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "surrounded-bunny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 대망의 텐서플로우!\n",
    "import os\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-broadcasting",
   "metadata": {},
   "source": [
    "### 정제 데이터 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "looking-plumbing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> load up on guns , bring your friends <end>',\n",
       " '<start> it s fun to lose and to pretend <end>',\n",
       " '<start> she s over bored and self assured <end>',\n",
       " '<start> oh no , i know a dirty word hello , hello , hello , how low <end>',\n",
       " '<start> hello , hello , hello , how low <end>',\n",
       " '<start> hello , hello , hello , how low <end>',\n",
       " '<start> hello , hello , hello with the lights out , it s less dangerous <end>',\n",
       " '<start> here we are now , entertain us <end>',\n",
       " '<start> i feel stupid and contagious <end>',\n",
       " '<start> here we are now , entertain us <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-moore",
   "metadata": {},
   "source": [
    "### 데이터를 숫자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regulation-projector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 2347   29 ...    0    0    0]\n",
      " [   2   11   16 ...    0    0    0]\n",
      " [   2   48   16 ...    0    0    0]\n",
      " ...\n",
      " [   2  135   50 ...    0    0    0]\n",
      " [   2  135    4 ...    0    0    0]\n",
      " [   2  135    4 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fdd9fe22fd0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=15000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "    tensor_15 = []\n",
    "    for voca in tensor:\n",
    "        if len(voca) < 16:    # 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외하도록 설정\n",
    "            tensor_15.append(voca)\n",
    "            \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor_15, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "otherwise-guinea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 2347   29   18 1100    4  307   21  262    3    0    0    0    0\n",
      "     0]\n",
      " [   2   11   16  531   10  384    8   10 1019    3    0    0    0    0\n",
      "     0]\n",
      " [   2   48   16  146 2963    8 1202 6481    3    0    0    0    0    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "electronic-blood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "modular-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2 2347   29   18 1100    4  307   21  262    3    0    0    0    0]\n",
      "[2347   29   18 1100    4  307   21  262    3    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-apple",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-chancellor",
   "metadata": {},
   "source": [
    "## Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "other-curtis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124810, 14)\n",
      "Target Train: (124810, 14)\n",
      "Source Train: (31203, 14)\n",
      "Target Train: (31203, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input,test_size=0.2,shuffle=True)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(\"Source Train:\", enc_val.shape)\n",
    "print(\"Target Train:\", dec_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stylish-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "scientific-failing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "interesting-cleveland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-newcastle",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-concept",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-jumping",
   "metadata": {},
   "source": [
    "## Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-stretch",
   "metadata": {},
   "source": [
    "### embedding_size = 256 / hidden_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "unable-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "polar-crazy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 15001), dtype=float32, numpy=\n",
       "array([[[ 1.20993071e-04, -9.35731896e-06, -5.16455002e-05, ...,\n",
       "          5.54307953e-05, -6.00745552e-05,  2.16923654e-04],\n",
       "        [ 1.39268755e-04,  4.22480916e-05, -1.44037287e-04, ...,\n",
       "          3.05972848e-04, -1.55333910e-04,  1.47773084e-04],\n",
       "        [ 3.79328907e-04,  7.70818369e-05, -3.76106793e-04, ...,\n",
       "          3.03419831e-04, -2.75298604e-04,  3.62912215e-05],\n",
       "        ...,\n",
       "        [ 4.54056681e-05,  5.91771328e-04, -3.93880706e-04, ...,\n",
       "         -3.76343756e-04,  1.18543336e-04, -2.89995281e-04],\n",
       "        [ 9.73325004e-05,  6.12279458e-04, -6.07421680e-04, ...,\n",
       "         -6.53054216e-04, -3.22883978e-04, -6.21841871e-04],\n",
       "        [ 2.40462483e-04,  5.73745230e-04, -8.71937722e-04, ...,\n",
       "         -9.76615702e-04, -7.72480620e-04, -1.00003695e-03]],\n",
       "\n",
       "       [[ 1.20993071e-04, -9.35731896e-06, -5.16455002e-05, ...,\n",
       "          5.54307953e-05, -6.00745552e-05,  2.16923654e-04],\n",
       "        [ 2.65595008e-04, -1.35764436e-04,  1.54859299e-05, ...,\n",
       "          1.58107490e-04, -1.51518077e-04,  2.28570498e-04],\n",
       "        [ 4.14613285e-04, -3.34716169e-04,  1.38476971e-04, ...,\n",
       "          2.52825586e-04, -2.86517665e-04,  1.64806566e-04],\n",
       "        ...,\n",
       "        [ 1.19700632e-03,  4.80069939e-05, -1.14903599e-03, ...,\n",
       "         -1.86474680e-03, -1.27262552e-03, -1.94734801e-03],\n",
       "        [ 1.35672314e-03, -1.28105166e-05, -1.46029680e-03, ...,\n",
       "         -2.18975008e-03, -1.52900955e-03, -2.26680702e-03],\n",
       "        [ 1.51126611e-03, -1.08671040e-04, -1.76061317e-03, ...,\n",
       "         -2.47292453e-03, -1.76953571e-03, -2.50364747e-03]],\n",
       "\n",
       "       [[ 1.20993071e-04, -9.35731896e-06, -5.16455002e-05, ...,\n",
       "          5.54307953e-05, -6.00745552e-05,  2.16923654e-04],\n",
       "        [ 1.62050361e-04, -1.27532301e-04,  1.14562818e-05, ...,\n",
       "         -7.57349289e-06, -1.40243530e-04,  2.88910873e-04],\n",
       "        [ 3.60212551e-04, -1.78823146e-04, -6.68552966e-05, ...,\n",
       "          6.85337682e-06, -8.50175420e-05,  2.35265907e-05],\n",
       "        ...,\n",
       "        [ 6.01192878e-04,  3.89064779e-04, -5.30849095e-04, ...,\n",
       "         -2.24431977e-03, -7.17749353e-04, -1.18722336e-03],\n",
       "        [ 8.38886772e-04,  3.54224845e-04, -7.54958193e-04, ...,\n",
       "         -2.59621418e-03, -9.97986062e-04, -1.62623846e-03],\n",
       "        [ 1.08166307e-03,  2.64800095e-04, -1.02044304e-03, ...,\n",
       "         -2.91248271e-03, -1.27723499e-03, -1.99635886e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.20993071e-04, -9.35731896e-06, -5.16455002e-05, ...,\n",
       "          5.54307953e-05, -6.00745552e-05,  2.16923654e-04],\n",
       "        [ 3.81003716e-04, -6.32061783e-05, -2.21786569e-04, ...,\n",
       "          1.47830608e-04,  2.69060638e-05,  9.47686658e-06],\n",
       "        [ 5.45547169e-04, -1.77513852e-04, -2.70002522e-04, ...,\n",
       "          4.03432059e-05,  8.46330731e-05, -2.82454246e-04],\n",
       "        ...,\n",
       "        [ 2.02383555e-04,  6.48827234e-04,  2.41091824e-04, ...,\n",
       "         -3.49406502e-04, -4.70931591e-05,  4.34852554e-04],\n",
       "        [ 2.24562842e-04,  7.17192073e-04,  2.20570146e-04, ...,\n",
       "         -4.91142622e-04, -3.38882644e-04,  2.15475142e-04],\n",
       "        [ 3.89234890e-04,  7.21597229e-04,  7.07054714e-05, ...,\n",
       "         -7.49480270e-04, -6.84321276e-04, -1.79970826e-04]],\n",
       "\n",
       "       [[ 1.20993071e-04, -9.35731896e-06, -5.16455002e-05, ...,\n",
       "          5.54307953e-05, -6.00745552e-05,  2.16923654e-04],\n",
       "        [ 2.88443043e-05,  1.36774848e-04, -1.63808028e-04, ...,\n",
       "          5.56443483e-05, -1.37526265e-04,  3.57477082e-04],\n",
       "        [ 1.31155830e-04,  3.46654531e-04, -4.05341794e-04, ...,\n",
       "          1.26332568e-04,  2.01039191e-04,  6.37212303e-04],\n",
       "        ...,\n",
       "        [ 2.94943660e-04, -5.84537629e-05, -1.20321068e-03, ...,\n",
       "         -1.19560934e-03, -4.76030284e-04, -1.35062414e-03],\n",
       "        [ 5.90502284e-04, -2.25990138e-04, -1.47427898e-03, ...,\n",
       "         -1.57218764e-03, -8.64116533e-04, -1.78660336e-03],\n",
       "        [ 8.74398567e-04, -3.98661476e-04, -1.75138621e-03, ...,\n",
       "         -1.91687420e-03, -1.21972489e-03, -2.13171612e-03]],\n",
       "\n",
       "       [[ 1.20993071e-04, -9.35731896e-06, -5.16455002e-05, ...,\n",
       "          5.54307953e-05, -6.00745552e-05,  2.16923654e-04],\n",
       "        [ 2.64546019e-04, -4.84524680e-05, -1.74428569e-04, ...,\n",
       "          1.83205368e-04, -1.86031291e-04,  4.36757313e-04],\n",
       "        [ 3.31943476e-04, -1.15460076e-04, -1.48229228e-04, ...,\n",
       "          1.02665304e-04, -1.01231220e-04,  7.27169216e-04],\n",
       "        ...,\n",
       "        [ 9.81602352e-06,  5.80467342e-04, -6.66030566e-04, ...,\n",
       "         -1.69158238e-03, -1.24174822e-03, -1.72696391e-03],\n",
       "        [ 2.76923965e-04,  4.12455352e-04, -9.95915616e-04, ...,\n",
       "         -2.06505228e-03, -1.49450032e-03, -2.11658352e-03],\n",
       "        [ 5.45943563e-04,  2.14614192e-04, -1.32580288e-03, ...,\n",
       "         -2.39292160e-03, -1.73101109e-03, -2.42326478e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in train_dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "divided-antenna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      multiple                  3840256   \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  15376025  \n",
      "=================================================================\n",
      "Total params: 32,855,961\n",
      "Trainable params: 32,855,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "beginning-pepper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 74s 152ms/step - loss: 3.5664 - val_loss: 3.1793\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 75s 153ms/step - loss: 3.0487 - val_loss: 2.9977\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 75s 154ms/step - loss: 2.8897 - val_loss: 2.8915\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 77s 158ms/step - loss: 2.7689 - val_loss: 2.8146\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 76s 156ms/step - loss: 2.6656 - val_loss: 2.7562\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 75s 154ms/step - loss: 2.5723 - val_loss: 2.7032\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 75s 154ms/step - loss: 2.4847 - val_loss: 2.6606\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 76s 155ms/step - loss: 2.4028 - val_loss: 2.6270\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 76s 157ms/step - loss: 2.3263 - val_loss: 2.5977\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 75s 154ms/step - loss: 2.2554 - val_loss: 2.5705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f657345d090>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "beneficial-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "large-degree",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m a liability <end> '"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-sponsorship",
   "metadata": {},
   "source": [
    "### embedding_size = 512 / hidden_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "committed-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "violent-consensus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      multiple                  7680512   \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               multiple                  6295552   \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  15376025  \n",
      "=================================================================\n",
      "Total params: 37,744,793\n",
      "Trainable params: 37,744,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for src_sample, tgt_sample in train_dataset.take(1): break\n",
    "model(src_sample)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "preliminary-carpet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 85s 174ms/step - loss: 3.4809 - val_loss: 3.1359\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 85s 174ms/step - loss: 3.0100 - val_loss: 2.9567\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 85s 174ms/step - loss: 2.8414 - val_loss: 2.8471\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 85s 175ms/step - loss: 2.7120 - val_loss: 2.7664\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 85s 175ms/step - loss: 2.6005 - val_loss: 2.7058\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 85s 175ms/step - loss: 2.5002 - val_loss: 2.6579\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 85s 174ms/step - loss: 2.4078 - val_loss: 2.6166\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 85s 174ms/step - loss: 2.3221 - val_loss: 2.5794\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 85s 175ms/step - loss: 2.2414 - val_loss: 2.5494\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 86s 177ms/step - loss: 2.1645 - val_loss: 2.5254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6570ca9690>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "under-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "palestinian-arnold",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-marketplace",
   "metadata": {},
   "source": [
    "### embedding_size = 128 / hidden_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dietary-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 128\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "heard-oxygen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     multiple                  1920128   \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               multiple                  4722688   \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             multiple                  15376025  \n",
      "=================================================================\n",
      "Total params: 30,411,545\n",
      "Trainable params: 30,411,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for src_sample, tgt_sample in train_dataset.take(1): break\n",
    "model(src_sample)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "disturbed-toronto",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 70s 144ms/step - loss: 3.5661 - val_loss: 3.2329\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 71s 145ms/step - loss: 3.0964 - val_loss: 3.0298\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 71s 146ms/step - loss: 2.9290 - val_loss: 2.9282\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 71s 146ms/step - loss: 2.8117 - val_loss: 2.8522\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 74s 153ms/step - loss: 2.7133 - val_loss: 2.7949\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 72s 147ms/step - loss: 2.6260 - val_loss: 2.7452\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 71s 146ms/step - loss: 2.5460 - val_loss: 2.7077\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 71s 146ms/step - loss: 2.4714 - val_loss: 2.6760\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 73s 149ms/step - loss: 2.4009 - val_loss: 2.6435\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 72s 147ms/step - loss: 2.3334 - val_loss: 2.6200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f650c663a50>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "arctic-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "intermediate-rolling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you so much <end> '"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-cleaner",
   "metadata": {},
   "source": [
    "### embedding_size = 256 / hidden_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "accessory-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "right-brand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      multiple                  3840256   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                multiple                  18882560  \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  30737049  \n",
      "=================================================================\n",
      "Total params: 87,022,489\n",
      "Trainable params: 87,022,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for src_sample, tgt_sample in train_dataset.take(1): break\n",
    "model(src_sample)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "persistent-sentence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 208s 427ms/step - loss: 3.5155 - val_loss: 3.1006\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 211s 434ms/step - loss: 2.9830 - val_loss: 2.8991\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 210s 432ms/step - loss: 2.7694 - val_loss: 2.7554\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 214s 440ms/step - loss: 2.5848 - val_loss: 2.6515\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 211s 433ms/step - loss: 2.4156 - val_loss: 2.5692\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 213s 437ms/step - loss: 2.2576 - val_loss: 2.5021\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 217s 445ms/step - loss: 2.1079 - val_loss: 2.4468\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 212s 435ms/step - loss: 1.9652 - val_loss: 2.3973\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 211s 433ms/step - loss: 1.8287 - val_loss: 2.3604\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 214s 439ms/step - loss: 1.6975 - val_loss: 2.3291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6574669c50>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "realistic-modern",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love frighten frighten sew fitted fitted memphis memphis tyiest tyiest tyiest pastor golden mighty vamos vamos claustrophobia claustrophobia '"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-attack",
   "metadata": {},
   "source": [
    "### embedding_size = 512 / hidden_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "split-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "occupied-format",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  7680512   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  20979712  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  30737049  \n",
      "=================================================================\n",
      "Total params: 92,959,897\n",
      "Trainable params: 92,959,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for src_sample, tgt_sample in train_dataset.take(1): break\n",
    "model(src_sample)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adjustable-russia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 216s 444ms/step - loss: 3.3634 - val_loss: 2.9506\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 218s 448ms/step - loss: 2.7746 - val_loss: 2.6960\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 219s 450ms/step - loss: 2.4438 - val_loss: 2.5025\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 220s 452ms/step - loss: 2.1140 - val_loss: 2.3609\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 220s 451ms/step - loss: 1.8116 - val_loss: 2.2592\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 220s 452ms/step - loss: 1.5539 - val_loss: 2.1976\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 220s 451ms/step - loss: 1.3469 - val_loss: 2.1713\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 219s 450ms/step - loss: 1.1914 - val_loss: 2.1672\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 219s 451ms/step - loss: 1.0903 - val_loss: 2.1840\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 221s 453ms/step - loss: 1.0309 - val_loss: 2.2044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdd2c5af690>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ready-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "moderate-train",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m not gonna crack <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-massachusetts",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-complement",
   "metadata": {},
   "source": [
    "## 루브릭 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-inspiration",
   "metadata": {},
   "source": [
    "__1. 가사 텍스트 생성 모델이 정상적으로 동작하는가?__  \n",
    "_(텍스트 제너레이션 결과가 그럴듯한 문장으로 생성되는가?)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-litigation",
   "metadata": {},
   "source": [
    "- 가사 텍스트 생성 모델이 정상적으로 동작하고 텍스트 제너레이션 결과 정상적인 문장이 작성되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-clothing",
   "metadata": {},
   "source": [
    "__2. 데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가?__  \n",
    "_(특수문자 제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되었는가?)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-diesel",
   "metadata": {},
   "source": [
    "- 특수문자 제거, 토크나이저 생성, 패딩처리 등 과정이 빠짐없이 진행되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-coordinate",
   "metadata": {},
   "source": [
    "__3. 텍스트 생성모델이 안정적으로 학습되었는가?__  \n",
    "_(텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-thanksgiving",
   "metadata": {},
   "source": [
    "- embedding_size = 512, hidden_size = 2048로 진행을 했을 경우 validation loss를 2.2까지 낮출 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-cricket",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-following",
   "metadata": {},
   "source": [
    "## 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-equation",
   "metadata": {},
   "source": [
    "__1. 프로젝트 진행시 어려웠던 점__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-reason",
   "metadata": {},
   "source": [
    "- 문장을 토큰화 했을 때 토크의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외하기 위한 코드를 수정한 점\n",
    "- validation loss를 2.2 이하로 낮추기 위해 하이퍼파라미터를 높게 설정해 모델을 학습 하다보니 많은 시간이 소요된 점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-covering",
   "metadata": {},
   "source": [
    "__2. 프로젝트를 진행하면서 알아낸 점 또는 아직 모호한 점__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-genesis",
   "metadata": {},
   "source": [
    "- 알아낸 점 :\n",
    "    - 하이퍼파라미터 조절에 따른 loss 변화\n",
    "    - 하이퍼파라미터에 따른 생성된 가사의 자연스러움\n",
    "- 모호한 점 :  \n",
    "    - hidden_size를 수정했을 때에는 뚜렷한 변화가 보였으나 embedding_size 수정시에는 큰 변화가 보이지 않아 어떤 영향을 주었는지 모호하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-substitute",
   "metadata": {},
   "source": [
    "__3. 루브릭 평가 지표를 맞추기 위해 시도한 점__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-regard",
   "metadata": {},
   "source": [
    "- embedding_size, hidden_size를 여러번 조절하여 validation loss 2.2로 낮출 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-ladder",
   "metadata": {},
   "source": [
    "__4. 루브릭 평가 지표를 달성하지 못한 이유__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-notice",
   "metadata": {},
   "source": [
    "- 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-tribune",
   "metadata": {},
   "source": [
    "__5. 프로젝트 진행 후 느낀 점 및 다짐__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-operation",
   "metadata": {},
   "source": [
    "- 모델 학습을 위한 최적의 하이퍼파라미터의 중요성을 다시 한 번 깨달았다.\n",
    "- 모델 학습시 정확도는 물론 시간을 줄이기 위한 방법을 알아봐야 겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
